# üóÇÔ∏è get_rid_of_dup.py: Eliminate Duplicate Files with Ease!

`get_rid_of_dup.py` is a command-line tool designed to find and remove duplicate files efficiently using checksum comparisons. Whether you're searching, managing, or deleting duplicate files, this script has got you covered! üíæ

This script has two modes:

**Mode 1: Single Directory Duplicate Detection**

One command is enough, it will identify all duplicates and ask confirmation to delete: 

```
python get_rid_of_dup.py dedup --base-dir ./comp --max-width 50 --verbose --exclude "*.DS_Store"
```

**Mode 2: Cross-Directory Duplicate Detection**

Two steps required:

1. Generate checksums.txt by using `checksum` option:

```
python get_rid_of_dup.py checksum --base-dir ./base ./comp --max-width 50   --verbose --exclude "*.DS_Store"
```

2. Based on `checksums.txt` which generated from above, deleting files:

```
python get_rid_of_dup.py delete --base-dir ./base ./comp --max-width 50   --verbose --exclude "*.DS_Store"
```


---

## üöÄ Features

Essentially, both modes are doing the same thing as below:

- **`search`**: Locate duplicate files without saving checksum information. This is usually for quick test.
- **`checksum`**: Everything in `search`, plus saving checksums into file. This is the first step in **Mode 2: Cross-Directory Duplicate Detection**.
- **`delete`**: Remove duplicate files based on checksum data (Default reading from checksums.txt). This is the second step in **Mode 2: Cross-Directory Duplicate Detection**.

---

## üì¶ Installation
I am using `Python 3.12.6`, it should work for all `Python 3+` as long as the additional packages installed successfully.

Run the following command to install the dependencies:

```pip install xxhash termcolor texttable```

## Considerations

### Determine which file as the original file

When hit duplicates, the script will use the file whose file name is shorter as the original file and mark others as the target to be deleted.

The reason why I designed it like this is that, I found usually the duplicates have names like "IMG_5808 (2).JPG". So only the original file has "IMG_5808.JPG" as file name which is shorter.

### Performance

The most time consuming is checksum command. 

But with `xxhash`, it's fast enough from my perspectiev.

I have around 30k files to scan, including many larger than 20MB images, it took just less than 1min to finish!

Another good thing of this script is that it supports to read checksums.txt file if specified `--skip-existing`.

Let's say you have scanned 10k files and saved in checksums.txt.

Then you added another 10k files, as long as you're in the same path, it will skip calculating whatever already in checksums.txt.

Below is my testing data:

- When first time to scan, it took around 84.46s:
![performance-01](asset/img/performance-01.webp)
- When second time to scan without adding any new file, it took around 
![performance-02](asset/img/performance-02.webp)
- If I deleted most checksums from `checksums.txt`, it took around . And we can see it's calculating only the missing ones. So it did skip existing files which already have checksums in `checksums.txt`.
![performance-03](asset/img/performance-03.webp)

Please note: `--skip-existing` is off by default, you have to specify it manaully.

## üìú Commands and Usage

### üîç `search` Command
Search for duplicate files by comparing against a base directory.

**Usage:**

```
python get_rid_of_dup.py search [options] <path>
```

**Options:**
- `<path>`: Path to search for duplicates (positional argument).
- `--base-dir`: Base directory containing original files (required).
- See [Common Arguments](#-common-arguments) below.

**Example:**

```
python get_rid_of_dup.py search --base-dir ./test ./others --max-width 50
```

---

### üîë `checksum` Command
Calculate checksums and optionally save them to a file while identifying duplicates.

**Usage:**

```
python get_rid_of_dup.py checksum [options] <path>
```

**Options:**
- `<path>`: Path to scan for duplicates (positional argument).
- `--update-checksum-file`: Update the checksum file with new entries.
- `--base-dir`: Base directory containing original files (required).
- See [Common Arguments](#-common-arguments) below.

**Example:**

```
python get_rid_of_dup.py checksum --no-update-checksum-file  --base-dir ./test  --max-width 50  ./others
```

---

### üóëÔ∏è `delete` Command
Delete duplicate files using checksum information.

**Usage:**

```
python get_rid_of_dup.py delete [options] <path>
```

**Options:**
- `<path>`: Path to delete duplicates from (positional argument).
- `--sleep-time`: Time to wait between deletions (default: 1.0 seconds).
- `--confirm`: Enable confirmation before deletion.
- `--list-next`: Number of files to preview before confirmation (default: 5).
- See [Common Arguments](#-common-arguments) below.

**Example:**

```
python get_rid_of_dup.py delete  --base-dir ./test ./others
```

---

## ‚öôÔ∏è Common Arguments
These arguments apply to all commands:

- `--base-dir`: **(Required)** Base directory containing original files.
- `--checksum-file`: File to save or read checksum data from (default: `checksums.txt`).
- `--skip-existing`: Skip checksum calculations for already-processed files. (default: False).
- `--no-skip-existing`: The opposite of `--skip-existing`.
- `--max-width`: Maximum column width for displaying results (default: 128).
- `--verbose`: Enable verbose output (default: False).
- `--no-verbose`: The opposite of `--verbose`.
- `--output-file`: Save duplicate files table to a file (default: dupfiles.txt).
- `--print-table`: Print the duplicate files table to the console (default: False).
- `--no-print-table`: The opposite of `--print-table`.

## Command-Specific Options:
- `checksum`: 
  - `--update-checksum-file`: Instruct to generate checksums file. Default filename is `checksums.txt`.
- `delete`: 
  - `--sleep-time`: Set delay between deletions (default: 1).
  - `--confirm`: Enable confirmation prompts (default: True).
  - `--no-confirm`: The opposite of `--confirm`
  - `--list-next`: Set number of files to preview before confirmation (default: 5).

---

## üí° Examples
### Search for Duplicates Without Saving Checksums into File

```
python get_rid_of_dup.py search --base-dir ./test ./others --max-width 50
```

![search-basic-usage](asset/img/search-01.webp)

The duplicate files table:

![dupfiles.txt.webp](asset/img/dupfiles.txt.webp)

```
python get_rid_of_dup.py search --base-dir ./test ./others --max-width 50 --verbose
```

![search-basic-usage](asset/img/search-02-with-verbose.webp)

### Search for Duplicates via Checksum and Save into File

```
python get_rid_of_dup.py checksum --base-dir ./test ./others --max-width 50
```

![checksum-01-save-checksum-default-behavior](asset/img/checksum-01-save-checksum-default-behavior.webp)


```
python get_rid_of_dup.py checksum --no-update-checksum-file --base-dir ./test ./others --max-width 50
```

![checksum-02-not-save-checksum-same-as-search-command](asset/img/checksum-02-not-save-checksum-same-as-search-command.webp)


### Delete Duplicates with Confirmation

```
python get_rid_of_dup.py delete  --base-dir ./test ./others
```

![delete-01](asset/img/delete-01.webp)

When missing checksums.txt:

```
python get_rid_of_dup.py delete  --base-dir ./test ./others
```

![delete-02](asset/img/delete-02.webp)


### Skip Existing Checksums with Verbose Output

```
python get_rid_of_dup.py checksum --base-dir /path/to/base_dir --skip-existing --verbose /path/to/other_dir
```

---

## üìù Notes
- Always **back up your data** before deleting files.
- Use `--confirm` with `delete` to review files before removal.
- The `--skip-existing` option speeds up processing for large datasets.
- Enable `--verbose` for detailed progress logs.

---

## ‚ù§Ô∏è Contribution & Support
Feel free to fork, contribute, or raise issues on GitHub! Suggestions, bug reports, and stars üåü are always welcome. Together, let's declutter your file system!

---
